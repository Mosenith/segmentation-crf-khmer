{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HMM-Khmer-Segmentaion.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "g0sDq1FdAjpc"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIzTSxIYGxtn",
        "colab_type": "text"
      },
      "source": [
        "## Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mwyuZ_GC3jo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This code below is mainly from\n",
        "https://github.com/jwchennlp/Chinese-Word-segmentation with small modification.\n",
        "\n",
        "Other notes and references:\n",
        "* https://nickchenyj.files.wordpress.com/2012/12/automatic-chinese-word-segmentation-with-hidden-markov-models-final.pdf -- not clear eval critera got 69% to 94%\n",
        "* A Pragmatic Approach for Classical Chinese Word Segmentation -- more recent include CRF- F-Score 76%\n",
        "https://www.aclweb.org/anthology/L18-1186\n",
        "* https://www.aclweb.org/anthology/W10-4128 HMM Revises Low Marginal Probability by CRF\n",
        "for Chinese Word Segmentation 97% accuracy\n",
        "* https://people.cs.umass.edu/~mccallum/papers/chineseseg.pdf - Chinese Word Segmentation with\n",
        "Conditional Random Fields and Integrated Domain Knowledge-- got 97% good paper to read on crf\n",
        "* https://github.com/keithnull/ChineseWordSegmentationSystem (not using the code, no model source code -- has flask for web )\n",
        "pku_test: 0.763 --> 0.829(the latest version)\n",
        "msr_test: 0.793 --> 0.889(the latest version) -- using F1 Score\n",
        "* http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.102.7060&rep=rep1&type=pdf - JAPANESE WORD SEGMENTATION BY HIDDEN MARKOV MODEL  -- 91% (https://www.aclweb.org/anthology/H94-1054)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEYaqfIKGl4k",
        "colab_type": "text"
      },
      "source": [
        "## Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxPpX3-W98_G",
        "colab_type": "text"
      },
      "source": [
        "Emission Matrix A: \n",
        "\n",
        "$P(X_k = w|Y_k = t) = A_{t,w}, \\forall k$\n",
        "\n",
        "| tag\\char | a   | e   | h   | i   | s   | t   |\n",
        "|      --- | --- | --- | --- | --- | --- | --- |\n",
        "| S        | 0.8 | --  | --  | --  | --  | --  |\n",
        "| B        | --  | --  | --  | 0.5 | --  | 0.4 |\n",
        "| M        | --  | 0.9 | 0.9 | 0.4 | 0.4 | --  |\n",
        "| E        | --  | --  | --  | --  | 0.3 | 0.5 |\n",
        "\n",
        "Transition Matrix B:\n",
        "\n",
        "$P(Y_k = t|Y_{k-1} = s) = B_{s,t}, \\forall k$\n",
        "\n",
        "| tag\\tag | S  | B  | M  | E  |\n",
        "|      -- | -- | -- | -- | -- |\n",
        "| S       | 0.1| 0.8| -- | -- |\n",
        "| B       | -- | -- | 0.5| 0.4|\n",
        "| M       | -- | -- | 0.4| 0.5|\n",
        "| E       | 0.3| 0.4| -- | -- |\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ_7p7WvwygU",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pzRH1IYPh5h",
        "colab_type": "code",
        "outputId": "2294c16a-1450-4651-d698-9582eb3e007b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Download the file from `url` and save it to outfile\n",
        "import urllib.request\n",
        "size = [\"100\",\"500\",\"1000\",\"5000\",\"10000_seg\"]\n",
        "# set file size for list of available size (0-5) that you want to use\n",
        "docsize = size[0]\n",
        "data_dir = \"kh_data_\" + docsize\n",
        "file_name = data_dir + \".zip\"\n",
        "base_url = \"https://github.com/phylypo/segmentation-crf-khmer/raw/master/data/\"\n",
        "url = base_url + file_name\n",
        "print(\"Downloading from:\", url)\n",
        "urllib.request.urlretrieve(url, file_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading from: https://github.com/phylypo/segmentation-crf-khmer/raw/master/data/kh_data_100.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('kh_data_100.zip', <http.client.HTTPMessage at 0x7f9c7ab2f630>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uX8sfRn1Uf4",
        "colab_type": "code",
        "outputId": "d42ae7eb-eeaf-4cca-a370-0cf6af0c2caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "print(\"- Unzipping the file and show last few extracted files:\")\n",
        "!unzip {file_name} | tail -10\n",
        "\n",
        "print(\"- Count the number of files:\")\n",
        "!ls -al {data_dir}/*_seg.txt | wc -l\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- Unzipping the file and show last few extracted files:\n",
            "  inflating: kh_data_100/313540_seg.txt  \n",
            "  inflating: kh_data_100/313541_orig.txt  \n",
            "  inflating: kh_data_100/313541_seg.txt  \n",
            "  inflating: kh_data_100/313544_orig.txt  \n",
            "  inflating: kh_data_100/313544_seg.txt  \n",
            "  inflating: kh_data_100/313545_orig.txt  \n",
            "  inflating: kh_data_100/313545_seg.txt  \n",
            "  inflating: kh_data_100/313546_orig.txt  \n",
            "  inflating: kh_data_100/313546_seg.txt  \n",
            "  inflating: kh_data_100/meta.txt    \n",
            "- Count the number of files:\n",
            "100\n",
            "kh_data_100  kh_data_100.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhfp40P2RXJl",
        "colab_type": "code",
        "outputId": "dea69791-3426-4897-ba69-21bf2752be27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Combine the content of files into one file for training and test\n",
        "#!ls -alh kh_data_100/313[34]*_seg.txt\n",
        "!cat kh_data_100/313[34]*_seg.txt > khmer_seg_train.txt\n",
        "!cat kh_data_100/3135*_seg.txt > khmer_seg_test.txt\n",
        "#!head khmer_seg_train.txt\n",
        "\n",
        "#total line: 982 , 35K words\n",
        "!wc khmer_seg_train.txt      # line count: 799, words: 28,287\n",
        "!wc khmer_seg_test.txt # line count: 182, words:  7,072"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   799  28287 416448 khmer_seg_train.txt\n",
            "   183   7072 101702 khmer_seg_test.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kptZmh3GPt1X",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpbwSDQ0CjKz",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class Model(object):\n",
        "    def __init__(self,states,observation,phi,trans_prob,conf_prob):\n",
        "        self._states = states\n",
        "        self._observation = observation\n",
        "        self._phi = phi\n",
        "        self._trans_prob = trans_prob\n",
        "        self._conf_prob = conf_prob\n",
        "\n",
        "    def states_length(self):\n",
        "        #Return the length of the states\n",
        "        return len(self._states)\n",
        "\n",
        "    def _forward(self,observations):\n",
        "        #The implemention of the forward algorithm\n",
        "        s_len = self.states_length\n",
        "        o_len = len(observations)\n",
        "        '''\n",
        "        This step should cal the alpha_t(j)\n",
        "        the t is the length of the observations,\n",
        "        the j is the hidden states\n",
        "        '''\n",
        "        alpha = [[] for i in range(o_len)]\n",
        "        \n",
        "        alpha[0] = {}\n",
        "        #t=1,cal the intil alpha_1(j)\n",
        "        for state in self._states:\n",
        "            alpha[0][state] = self._conf_prob[state][observations[0]]*self._phi[state]\n",
        "        \n",
        "        #t>1,cal the local prob alpha_t(j)\n",
        "        for index in range(1,o_len):\n",
        "            alpha[index] ={}\n",
        "            for state_to in self._states:\n",
        "                #the time t the prob all path that direct to states_to\n",
        "                prob = 0\n",
        "                for state_from in self._states:\n",
        "                    prob += alpha[index-1][state_from]*self._trans_prob[state_from][state_to]\n",
        "                alpha[index][state_to]=self._conf_prob[state_to][observations[index]]*prob\n",
        "        return alpha\n",
        "        \n",
        "    def _backward(self,observations):\n",
        "        #The implementation of the backward algorithm\n",
        "        s_len = self.states_length\n",
        "        o_len = len(observations)\n",
        "        '''\n",
        "        This step should cal the beta_t(j)\n",
        "        the t is the location of the observations,\n",
        "        the j is the hidden states\n",
        "        beta_t(j) = p(o_(t+1)...o_T|q_t=s_j,\\lambda)\n",
        "        '''\n",
        "        beta = [[] for i in range(o_len)] \n",
        "        beta[o_len-1] = {}\n",
        "        #t=T,the intial beta_T(j)\n",
        "        for state in self._states:\n",
        "            beta[o_len-1][state] = 1\n",
        "        \n",
        "        #t<T,cal the local prob beta_t(j)\n",
        "        index = len(observations)-1\n",
        "        while index > 0:\n",
        "            beta[index-1] = {}\n",
        "            for state_from in self._states:\n",
        "                prob = 0\n",
        "                for state_to in self._states:\n",
        "                    prob += self._trans_prob[state_from][state_to] * \\\n",
        "                        self._conf_prob[state_to][observations[index]]* \\\n",
        "                        beta[index][state_to]\n",
        "                beta[index-1][state_from] = prob\n",
        "            index -= 1\n",
        "        return beta\n",
        "        \n",
        "    def _viterbi(self,observations):\n",
        "        #The implemention of the viterbi algorithm\n",
        "        s_len = self.states_length\n",
        "        o_len = len(observations)\n",
        "        '''\n",
        "        This step should cal the beta_t(j),\n",
        "        the t is the length of the observations,\n",
        "        the j is the hidden states,\n",
        "        the beta_t(j) means at time t the most probable \n",
        "        local path to state j\n",
        "        '''\n",
        "        beta = [[] for i in range(o_len)]\n",
        "        beta[0] = {}\n",
        "        \n",
        "        for state in self._states:\n",
        "            beta[0][state] = self._conf_prob[state][observations[0]]*self._phi[state]\n",
        "            \n",
        "        #t>1,cal the local prob beta_t(j)\n",
        "        for index in range(1,o_len):\n",
        "            beta[index] = {}\n",
        "            for state_to in self._states:\n",
        "                #build a list to save the beta_t-1(j)a_jib_ikt\n",
        "                prob = []\n",
        "                for state_from in self._states:\n",
        "                    temp = beta[index-1][state_from]*self._trans_prob[state_from][state_to]*self._conf_prob[state_to][observations[index]]\n",
        "                    prob.append(temp)\n",
        "                prob =sorted(prob,reverse = True)\n",
        "                beta[index][state_to] = prob[0]\n",
        "        return beta\n",
        "    \n",
        "    def _backward_point(self,beta,observations,state):\n",
        "        \"\"\"\n",
        "        rely on the beta to get the state sequences that best \n",
        "        explain the observation sequences\n",
        "        \"\"\"\n",
        "        index = len(observations)-1\n",
        "        theta =[0 for i in range(len(observations))]\n",
        "        theta[index] = state\n",
        "        while index >0:\n",
        "            prob = {}\n",
        "            for state_from in self._states:\n",
        "                prob[state_from] = beta[index-1][state_from]*self._trans_prob[state_from][state]\n",
        "            state = sorted(prob,key=prob.get,reverse=True)[0]\n",
        "            index -= 1\n",
        "            theta[index] = state\n",
        "        return theta\n",
        "        \n",
        "    def _inverse(self,beta):\n",
        "        result = [0 for i in range(len(beta))] \n",
        "        length = len(beta)\n",
        "        for i in range(len(beta)):\n",
        "            result[i] = beta[length-i-1]\n",
        "        return result\n",
        "    \n",
        "    def _intial_par(self):\n",
        "        '''\n",
        "        phi,trans_prob,conf_prob = {},{},{}\n",
        "        N = len(self._states)\n",
        "        M = len(self._observation)\n",
        "        for state in self._states:\n",
        "            phi[state] = 1.0/N\n",
        "            trans_prob[state] = {}\n",
        "            for state_to in self._states:\n",
        "                trans_prob[state][state_to] = 1.0/N\n",
        "            conf_prob[state] = {}\n",
        "            for ob in self._observation:\n",
        "                conf_prob[state][ob] = 1.0/M\n",
        "        '''\n",
        "        phi = self._phi\n",
        "        trans_prob = self._trans_prob\n",
        "        conf_prob = self._conf_prob\n",
        "        return (phi,trans_prob,conf_prob)\n",
        "\n",
        "    def _cal_gamma(self,alpha,beta,observations):\n",
        "        T = len(observations)\n",
        "        gamma = [[] for x in range(T)]\n",
        "        for t in range(T):\n",
        "            gamma[t] = {}\n",
        "            sum_prob = 0\n",
        "            for state in self._states:\n",
        "                prob = alpha[t][state]*beta[t][state]\n",
        "                sum_prob += prob\n",
        "                gamma[t][state] = prob\n",
        "            for state in self._states:\n",
        "                if gamma[t][state] == 0:\n",
        "                    continue\n",
        "                else:\n",
        "                    gamma[t][state] /= sum_prob\n",
        "        return gamma\n",
        "        \n",
        "    def _cal_espi(self,alpha,beta,trans_prob,conf_prob,observations):\n",
        "        T = len(observations)\n",
        "        espi = [[] for x in range(T-1)]\n",
        "        for t in range(T-1):\n",
        "            espi[t] = {}\n",
        "            sum_prob = 0\n",
        "            for state_i in self._states:\n",
        "                espi[t][state_i] = {}\n",
        "                for state_j in self._states:\n",
        "                   prob = alpha[t][state_i]*trans_prob[state_i][state_j]*conf_prob[state_j][observations[t+1]]*beta[t+1][state_j]\n",
        "                   espi[t][state_i][state_j] = prob\n",
        "                   sum_prob += prob\n",
        "            for i in self._states:\n",
        "                for j in self._states:\n",
        "                    if espi[t][i][j] == 0:\n",
        "                        continue\n",
        "                    else:\n",
        "                        espi[t][i][j] /= sum_prob\n",
        "        return espi\n",
        "        \n",
        "    def _evaluate_par(self,gamma,espi,observations):\n",
        "        T = len(observations)\n",
        "        phi = gamma[0]\n",
        "        trans_prob,conf_prob = {},{}\n",
        "        for state in self._states:\n",
        "            trans_prob[state] = {}\n",
        "            conf_prob[state] = {}\n",
        "        for i in self._states:\n",
        "            for j in self._states:\n",
        "                gamma_t,espi_t = 0,0\n",
        "                for t in range(T-1):\n",
        "                    espi_t += espi[t][i][j]\n",
        "                    gamma_t += gamma[t][i]\n",
        "                trans_prob[i][j] = espi_t/gamma_t\n",
        "        for state in self._states:\n",
        "            for o in self._observation:\n",
        "                gamma_con_t ,gamma_t = 0,0\n",
        "                for t in range(T):\n",
        "                    if observations[t] == o:\n",
        "                        gamma_con_t += gamma[t][state]\n",
        "                    gamma_t = gamma[t][state]\n",
        "                conf_prob[state][o] = gamma_con_t/gamma_t\n",
        "        return (phi,trans_prob,conf_prob)\n",
        "\n",
        "    \n",
        "    def evaluate(self,observations):\n",
        "        \"\"\"\n",
        "        use the forward algorithm to cal the \n",
        "        prob of the observation sequence under the HMM Model\n",
        "        \"\"\"\n",
        "        length = len(observations)\n",
        "        if length == 0:\n",
        "            return 0\n",
        "        \n",
        "        alpha = self._forward(observations)\n",
        "        prob = sum(alpha[length-1].values())\n",
        "        return prob\n",
        "        \n",
        "    def decode(self,observations):\n",
        "        \"\"\"\n",
        "        user the be viterbi algorithm to cal the most probable \n",
        "        hidden state sequence to the observations sequence ,\n",
        "        \"\"\"\n",
        "        length = len(observations)\n",
        "        if length == 0 :\n",
        "            return 0\n",
        "        beta = self._viterbi(observations)\n",
        "        #get the last state to the last obseravtions\n",
        "        sequence = beta[length-1]\n",
        "        state = sorted(sequence,key=sequence.get,reverse=True)[0]\n",
        "        theta = self._backward_point(beta,observations,state)\n",
        "        return theta\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz5RdDEJDGtC",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVB6CtDPDKKL",
        "colab_type": "code",
        "outputId": "6ee194be-3628-4cff-e138-8077f59f0bdf",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title pre-process code -expand to see detail\n",
        "import codecs,re\n",
        "import sys\n",
        "\n",
        "class Process(object):\n",
        "    def __init__(self,file_dir,S):\n",
        "        self._file_dir = file_dir\n",
        "        self._S = S\n",
        "        self.labels =[]\n",
        "\n",
        "    def _str2words(self,test):\n",
        "        words =[]\n",
        "        x=codecs.lookup(\"utf-8\")\n",
        "        for string in test:\n",
        "            word = x.decode(string[0])[0]\n",
        "            words.append(word)\n",
        "        return words\n",
        "\n",
        "    def _statics(self):\n",
        "        f = codecs.open(self._file_dir,'rb',encoding = 'utf-8')\n",
        "        hidden_states,train = [],[]\n",
        "        for line in f.readlines():\n",
        "            '''\n",
        "            First make tag for the tokenize in the corpus\n",
        "            '''\n",
        "            hidden_state = ''\n",
        "            words = []\n",
        "            # clean up text -- remove 2 spaces to 1, and invisible spaces\n",
        "            line = line.strip()\n",
        "            line = line.replace(\"  \",\" \")\n",
        "            line = line.replace('\\u200b','') \n",
        "            line = line.replace('\\r\\n','')\n",
        "            tokenizes = line.split()\n",
        "            for token in tokenizes:\n",
        "                length = len(token)\n",
        "                if length == 1:\n",
        "                    hidden_state += 'S'\n",
        "                elif length==2:\n",
        "                    hidden_state += 'BE'\n",
        "                else:\n",
        "                    hidden_state += 'B'+(length-2)*'M'+'E'\n",
        "            '''\n",
        "            Second we should extart single character from the corpus\n",
        "            '''\n",
        "            line = line.replace(' ','') # remove space\n",
        "            #print(\"--line:\", line)\n",
        "            for word in line:\n",
        "                words.append(word) # this is character, not word\n",
        "                # can use kcc here\n",
        "            if len(words) >0:\n",
        "                train.append(words)\n",
        "                hidden_states.append(hidden_state)\n",
        "        print(\"process._statics: \",self._file_dir, \" total word count:\", len(train), \" len hidden_state:\", len(hidden_states))\n",
        "        print(\"process._statics: \",self._file_dir, \" first word/line:\", train[0], \" len hidden_state:\", len(hidden_states))\n",
        "        return (hidden_states,train)\n",
        "            \n",
        "    def _statics_hidden(self):\n",
        "        '''\n",
        "        First,get the tokenize result of the corpus,\n",
        "        statics the hidden state of each word\n",
        "        '''\n",
        "        f = open(self._file_dir,'rb')\n",
        "        hidden_states,train = [],[]\n",
        "        regex=re.compile(\"(?x) ( [\\w-]+ | [\\x80-\\xff]{3} )\")\n",
        "        for line in f.readlines():\n",
        "            hidden_state = ''\n",
        "            words = []\n",
        "            tokenizes = line.split()\n",
        "            for token in tokenizes:\n",
        "                temp = [w for w in regex.split(token) if w]\n",
        "                for t in temp:\n",
        "                    words.append(t)\n",
        "                length = len(temp)\n",
        "                if length == 1:\n",
        "                    hidden_state += 'S'\n",
        "                elif length==2:\n",
        "                    hidden_state += 'BE'\n",
        "                else:\n",
        "                    hidden_state += 'B'+(length-2)*'M'+'E'\n",
        "            if len(words) != 0:\n",
        "                train.append(words)\n",
        "                hidden_states.append(hidden_state)\n",
        "        return (hidden_states,train)\n",
        "\n",
        "            \n",
        "    def _word_count(self,train):\n",
        "        word_count = {}\n",
        "        for words in train:\n",
        "            for word in words:\n",
        "                if word in word_count.keys(): #word_count.has_key(word):\n",
        "                    word_count[word] += 1\n",
        "                else:\n",
        "                    word_count[word] = 1\n",
        "        return word_count\n",
        "    \n",
        "    def _convert(self,hidden_states):\n",
        "        temp = []\n",
        "        for index in range(len(hidden_states)):\n",
        "            regex = re.compile(\"(\\w{1})\")\n",
        "            states = [w for w in regex.split(hidden_states[index]) if w]\n",
        "            if len(states) !=0:\n",
        "                temp.append(states)\n",
        "        return temp\n",
        "    \n",
        "    def _cal_trans(self,h_s):\n",
        "        trans_prob,state_count = {},{}\n",
        "        #intial\n",
        "        for state in self._S:\n",
        "            trans_prob[state]={}\n",
        "            state_count[state] = 0\n",
        "            for state_i in self._S:\n",
        "                trans_prob[state][state_i]=0\n",
        "        for i in range(len(h_s)):\n",
        "            length = len(h_s[i])\n",
        "            for j in range(length-1):\n",
        "                s_from = h_s[i][j]\n",
        "                s_to = h_s[i][j+1]\n",
        "                trans_prob[s_from][s_to] += 1\n",
        "                state_count[s_from] += 1\n",
        "            state_count[h_s[i][length-1]] += 1\n",
        "        print(state_count)\n",
        "        for i in self._S:\n",
        "            for j in self._S:\n",
        "                trans_prob[i][j] /= float(state_count[i])\n",
        "        return (trans_prob,state_count)\n",
        "    \n",
        "    def _cal_conf(self,h_s,test_wordcount,word_count,train,state_count):\n",
        "        conf_prob = {}\n",
        "        words = list(set(word_count.keys())|set(test_wordcount.keys()))\n",
        "        print('The corpus has distinct count %d word'%(len(words)))\n",
        "        for state in self._S:\n",
        "            conf_prob[state] = {}\n",
        "            for word in words:\n",
        "                conf_prob[state][word] = 1\n",
        "        for i in range(len(h_s)):\n",
        "            length = len(h_s[i])\n",
        "            for j in range(length):\n",
        "                obser = train[i][j]\n",
        "                hidden = h_s[i][j]\n",
        "                conf_prob[hidden][obser] += 1\n",
        "        for state in self._S:\n",
        "            for word in words:\n",
        "                if conf_prob[state][word] == 0:\n",
        "                    continue\n",
        "                else:\n",
        "                    conf_prob[state][word] /= float(state_count[state])\n",
        "        return conf_prob\n",
        "        \n",
        "\n",
        "    def _tran_conf_prob(self,train,test_wordcount,word_count,hidden_states):\n",
        "        #convert the hidden_state string to list\n",
        "        hidden_states = self._convert(hidden_states)\n",
        "        trans_prob,state_count = self._cal_trans(hidden_states)\n",
        "        conf_prob = self._cal_conf(hidden_states,test_wordcount,word_count,train,state_count)\n",
        "        \n",
        "        return (conf_prob,trans_prob)\n",
        "        \n",
        "    def _word_sequence(self,test,o_hstate):\n",
        "        sequence = []\n",
        "        f= open('result.txt','w')\n",
        "        print('word_seq len test:', len(test))\n",
        "        for i in range(len(test)):\n",
        "            if o_hstate[i][-1] == 'M':\n",
        "                o_hstate[i][-1] = 'E'\n",
        "            elif o_hstate[i][-1] == 'B':\n",
        "                o_hstate[i][-1] = 'S'\n",
        "            length = len(test[i])\n",
        "            temp = []\n",
        "            k = 0\n",
        "            #print(\"len(test[i]\",len(test[i]), \"len(o_hstate[i])\",len(o_hstate[i]))\n",
        "            while k < length:\n",
        "                if o_hstate[i][k]=='S':\n",
        "                    temp.append(test[i][k])\n",
        "                else :\n",
        "                    s=test[i][k]\n",
        "                    k+=1\n",
        "                    try:\n",
        "                      if k < len(o_hstate[i]): \n",
        "                        #add by phyl to check invalid index\n",
        "                        while k < len(o_hstate[i])-1 and o_hstate[i][k] != 'E' :\n",
        "                          s += test[i][k]\n",
        "                          k +=1\n",
        "                        s += test[i][k]\n",
        "                        temp.append(s)\n",
        "                    except: \n",
        "                      print(\"exception i:\",i, \"k:\", k, \"len(o_hstate)\", len(o_hstate), \"len(o_hstate[i])\", len(o_hstate[i]))\n",
        "                k += 1\n",
        "            f.write('%s\\n' %(' '.join(temp)))\n",
        "            #print(i, \" - len(temp):\", len(temp), (' - '.join(temp)))\n",
        "            sequence.append(' '.join(temp))\n",
        "        f.close()\n",
        "            \n",
        "        return sequence\n",
        "      \n",
        "print(\"[DONE]\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DONE]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLIyNfwltVWV",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9D8OkeiDZDg",
        "colab_type": "code",
        "outputId": "9ce91fbe-ae1e-4e86-ee89-55c02a45191e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "import codecs\n",
        "import sys\n",
        "\n",
        "#train_dir = '/content/icwb2-data/training/pku_training.utf8'\n",
        "#test_dir = '/content/icwb2-data/testing/pku_test.utf8'\n",
        "train_dir = 'khmer_seg_train.txt'\n",
        "test_dir = 'khmer_seg_test.txt'\n",
        "\n",
        "'''\n",
        "The number of the hidden states\n",
        "B:a word at the start\n",
        "E:a word at the end\n",
        "M:a word at the middle\n",
        "S:a word construct the tokenize\n",
        "'''\n",
        "\n",
        "S = ['B','E','M','S']\n",
        "pro = Process(train_dir,S)\n",
        "hidden_states,train=pro._statics()\n",
        "\n",
        "pro_test = Process(test_dir,S)\n",
        "test_states,test = pro_test._statics()\n",
        "\n",
        "test_wordcount = pro_test._word_count(test)\n",
        "word_count = pro._word_count(train)\n",
        "\n",
        "observation = word_count.keys()\n",
        "print(\"test state len:\",len(test_states), \"test states0\",test_states[0])\n",
        "print(\"Observation:\", len(observation), observation)\n",
        "\n",
        "'''\n",
        "The conf_prob is the probability of a observation in condition of a hidden state\n",
        "The trans_prob is the probability of a  hidden state trans to another\n",
        "This time add the smoothing method.\n",
        "1.add 1 mehtod\n",
        "'''\n",
        "conf_prob,trans_prob=pro._tran_conf_prob(train,test_wordcount,word_count,hidden_states)\n",
        "\n",
        "print('conf_prob', conf_prob)\n",
        "print('trans_prob', trans_prob)\n",
        "\n",
        "observations = test\n",
        "\n",
        "phi = {'B':0.4,'E':0.4,'M':0.1,'S':0.1} #Begin, End, Middle, Single\n",
        "model = Model(S,observation,phi,trans_prob,conf_prob)\n",
        "o_hstate = []\n",
        "\n",
        "print(\"- Preprocess: len of observation:\", len(observations), observations[0])\n",
        "for obser in observations:\n",
        "    '''\n",
        "    Notice,if a setence is too long,when we use viterbi algorithm it may result in the beta = 0\n",
        "    There are two solution,one is split the setence into serval sub_setence,another is use log function for the viterbi \n",
        "    here we select the first method\n",
        "    '''\n",
        "    length = len(obser)\n",
        "    index,sub_obser,state= 0,[],[]\n",
        "    # end of sentence -- we already break by sentence to new line, this is not necceary\n",
        "    END_TOKENS = ['。', ',','៕','។','?',')',\":\",\"\\\"\"]\n",
        "    while index < length:\n",
        "        sub_obser.append(obser[index])\n",
        "        if obser[index] in END_TOKENS: #obser[index] == '。' or obser[index]=='，':\n",
        "            sub_state = model.decode(sub_obser)\n",
        "            sub_obser = []\n",
        "            state += sub_state\n",
        "        elif index == length-1:\n",
        "            sub_state = model.decode(sub_obser)\n",
        "            sub_obser = []\n",
        "            state += sub_state\n",
        "        index += 1\n",
        "    o_hstate.append(state)\n",
        "\n",
        "word_sequence = pro._word_sequence(observations,o_hstate)\n",
        "print(\"-word_sequence[0]\", word_sequence[0])\n",
        "print(\"0-hstates0\", o_hstate[0])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "process._statics:  khmer_seg_train.txt  total word count: 799  len hidden_state: 799\n",
            "process._statics:  khmer_seg_train.txt  first word/line: ['ឃ', 'ា', 'ត', '់', 'ជ', 'ន', 'ស', 'ង', '្', 'ស', '័', 'យ', 'ម', '្', 'ន', 'ា', 'ក', '់', 'ប', 'ន', '្', 'ទ', 'ា', 'ប', '់', 'ព', 'ី', 'ធ', '្', 'វ', 'ើ', 'ស', 'ក', 'ម', '្', 'ម', 'ភ', 'ា', 'ព', 'ល', 'ួ', 'ច', 'យ', 'ក', 'ក', 'ា', 'ប', 'ូ', 'ប', 'ល', 'ុ', 'យ', 'ជ', 'ន', 'រ', 'ង', 'គ', '្', 'រ', 'ោ', 'ះ', 'ក', '្', 'ន', 'ុ', 'ង', 'ផ', '្', 'ស', 'ា', 'រ', 'ល', 'ើ', 'ធ', 'ំ', 'ថ', '្', 'ម', 'ី', 'ក', '្', 'រ', 'ុ', 'ង', 'ស', 'ៀ', 'ម', 'រ', 'ា', 'ប', 'ន', 'ិ', 'ង', 'ប', 'ា', 'ន', 'ដ', 'ក', 'ហ', 'ូ', 'ត', 'ប', 'ា', 'ន', 'ទ', '្', 'រ', 'ព', '្', 'យ', 'ស', 'ម', '្', 'ប', 'ត', '្', 'ត', 'ិ', 'ជ', 'ូ', 'ន', 'ជ', 'ន', 'រ', 'ង', 'គ', '្', 'រ', 'ោ', 'ះ', 'វ', 'ិ', 'ញ']  len hidden_state: 799\n",
            "process._statics:  khmer_seg_test.txt  total word count: 183  len hidden_state: 183\n",
            "process._statics:  khmer_seg_test.txt  first word/line: ['អ', 'ា', 'ម', 'េ', 'រ', 'ិ', 'ក', 'ព', '្', 'រ', 'ម', 'ផ', '្', 'ត', 'ល', '់', 'ស', 'េ', 'វ', 'ា', 'គ', 'ា', 'ំ', 'ទ', '្', 'រ', 'ប', 'ច', '្', 'ច', 'េ', 'ក', 'ទ', 'េ', 'ស', 'ដ', 'ល', '់', 'យ', 'ន', '្', 'ត', 'ហ', 'ោ', 'ះ', 'ច', 'ម', '្', 'ប', 'ា', 'ំ', 'ង', 'F', '-', '1', '6', 'ប', '៉', 'ា', 'គ', 'ី', 'ស', '្', 'ថ', 'ា', 'ន', 'ន', 'ិ', 'ង', 'ល', 'ក', '់', 'គ', '្', 'រ', 'ឿ', 'ង', 'ប', 'ន', '្', 'ល', 'ា', 'ស', '់', 'យ', 'ន', '្', 'ត', 'ហ', 'ោ', 'ះ', 'យ', 'ោ', 'ធ', 'ា', 'ឲ', '្', 'យ', 'ឥ', 'ណ', '្', 'ឌ', 'ា', '2', '7', ',', 'J', 'u', 'l', '2', '0', '1', '9', ',', '1', '0', ':', '3', '0', 'p', 'm']  len hidden_state: 183\n",
            "test state len: 183 test states0 BMMMMMEBMMEBMMMEBMMEBMMMMEBMMMMMMMEBMEBMMMMMEBMMMMMESSBEBMMMMMMMMEBMEBMEBMMMEBMMMMMEBMMMMMEBMMEBMEBMMMEBMEBMEBMMESBMMMEBE\n",
            "Observation: 167 dict_keys(['ឃ', 'ា', 'ត', '់', 'ជ', 'ន', 'ស', 'ង', '្', '័', 'យ', 'ម', 'ក', 'ប', 'ទ', 'ព', 'ី', 'ធ', 'វ', 'ើ', 'ភ', 'ល', 'ួ', 'ច', 'ូ', 'ុ', 'រ', 'គ', 'ោ', 'ះ', 'ផ', 'ំ', 'ថ', 'ៀ', 'ិ', 'ដ', 'ហ', 'ញ', ':', 'ៃ', '២', '៤', 'ខ', 'ែ', 'ឆ', '០', '១', '៩', 'េ', '៉', '៨', 'ឹ', 'ណ', 'ឈ', '៊', 'អ', '៥', '៦', 'ៅ', 'ៈ', '/', '។', 'ឱ', '៏', 'ឲ', '៍', 'ឋ', '៣', 'ឡ', '+', 'ឺ', 'ឧ', 'ឯ', 'ឌ', 'ឿ', '៧', '៖', '-', '៕', 'ឍ', '៌', '(', 'W', 'T', 'O', ')', '«', 'ៗ', '!', '»', 'ឬ', 'G', '2', '0', 'ឥ', 'M', 'a', 't', 'i', 'n', 'P', 'l', 'b', 'h', 'U', 'r', 'd', 'e', 's', 'V', 'S', 'v', '”', 'o', 'B', 'C', 'A', 'g', 'c', 'u', 'p', 'J', ',', '5', '4', '1', '.', '\"', 'ឪ', 'm', 'R', 'f', 'F', 'L', 'H', 'w', 'k', 'y', 'j', '“', 'K', 'I', 'N', 'D', '…', '–', 'X', '_', 'E', '?', '$', '9', 'Z', '8', '7', '6', '%', '3', 'Y', 'ឫ', 'z', '÷', '’', '©', '=', 'ឦ', 'x'])\n",
            "{'B': 26487, 'E': 26487, 'M': 75403, 'S': 1800}\n",
            "The corpus has distinct count 176 word\n",
            "conf_prob {'B': {'៏': 3.775437006833541e-05, 'v': 0.00015101748027334164, 'ិ': 3.775437006833541e-05, 'ឍ': 3.775437006833541e-05, '៊': 3.775437006833541e-05, '៩': 0.00011326311020500623, '3': 3.775437006833541e-05, '២': 0.008381470155170462, 'គ': 0.03175142522747008, 'ង': 0.0010948767319817268, '9': 0.00018877185034167705, 'ឈ': 0.004303998187790236, '៖': 3.775437006833541e-05, '$': 3.775437006833541e-05, 'g': 3.775437006833541e-05, '+': 3.775437006833541e-05, 'ត': 0.028240268811114886, 'ល': 0.02986370672405331, 'ឃ': 0.004945822478951939, 'Z': 3.775437006833541e-05, 'ព': 0.03771661569826707, 'ឆ': 0.009325329406878847, '6': 7.550874013667082e-05, 'ម': 0.06093555329029335, 'f': 7.550874013667082e-05, 'ស': 0.08328614037074791, 'H': 0.0004908068108883604, 'ឲ': 0.0022652622041001245, '=': 3.775437006833541e-05, 'W': 0.0005663155510250311, '–': 3.775437006833541e-05, 'E': 0.00018877185034167705, 'T': 0.0009816136217767207, 'ៗ': 3.775437006833541e-05, 'វ': 0.012043644051798997, '8': 0.00018877185034167705, '.': 0.0003397893306150187, '»': 0.00011326311020500623, '់': 3.775437006833541e-05, '័': 3.775437006833541e-05, 'ឺ': 3.775437006833541e-05, 'ឦ': 0.00011326311020500623, '៌': 3.775437006833541e-05, '“': 3.775437006833541e-05, 'L': 0.0005285611809566958, 'V': 0.0003775437006833541, 'I': 0.0003020349605466833, 'ធ': 0.011741609091252313, '÷': 3.775437006833541e-05, 'O': 0.00022652622041001246, 'ុ': 3.775437006833541e-05, 'u': 3.775437006833541e-05, '5': 7.550874013667082e-05, 'ឩ': 3.775437006833541e-05, 'ឫ': 0.00022652622041001246, 'É': 3.775437006833541e-05, 'ភ': 0.011552837240910635, 'ដ': 0.05410201230792464, 'ៅ': 3.775437006833541e-05, '៤': 0.0010193679918450561, '-': 3.775437006833541e-05, 'r': 3.775437006833541e-05, 't': 7.550874013667082e-05, 'N': 0.0005285611809566958, '៉': 3.775437006833541e-05, '%': 3.775437006833541e-05, 'ឱ': 0.0038131913769018765, 'រ': 0.045380752822139166, 'J': 0.0006795786612300374, 'ថ': 0.02771170763015819, 'e': 3.775437006833541e-05, 'ឯ': 0.0015856835428700873, ',': 3.775437006833541e-05, '©': 3.775437006833541e-05, '…': 0.00015101748027334164, 'ញ': 0.0006040699210933666, 'A': 0.0009061048816400498, 'ួ': 3.775437006833541e-05, 'ច': 0.03356363499075018, 'X': 0.00015101748027334164, '្': 3.775437006833541e-05, 'R': 0.0005285611809566958, 'ហ': 0.01415788877562578, 'ឡ': 0.0030581039755351682, 'ផ': 0.016725185940272586, '4': 3.775437006833541e-05, 'b': 3.775437006833541e-05, 'ោ': 3.775437006833541e-05, 'ឧ': 0.002114244723826783, 'ា': 3.775437006833541e-05, 'C': 0.0007550874013667082, 'ឹ': 3.775437006833541e-05, '/': 3.775437006833541e-05, 'P': 0.0012081398421867331, '៍': 3.775437006833541e-05, 'ឌ': 0.0002642805904783479, '0': 0.00015101748027334164, 'z': 3.775437006833541e-05, '៣': 0.0009061048816400498, 'o': 3.775437006833541e-05, 'ឭ': 3.775437006833541e-05, '«': 3.775437006833541e-05, '7': 0.00011326311020500623, '’': 3.775437006833541e-05, '2': 0.0007550874013667082, 'ី': 3.775437006833541e-05, 'G': 0.000641824291161702, '\"': 3.775437006833541e-05, '?': 3.775437006833541e-05, ')': 0.0002642805904783479, 'េ': 3.775437006833541e-05, 'ឪ': 0.0002642805904783479, 'ៃ': 3.775437006833541e-05, 'w': 0.00015101748027334164, '១': 0.0032468758258768454, '៥': 0.0009061048816400498, 'a': 7.550874013667082e-05, 'S': 0.0010571223619133916, 'ខ': 0.02986370672405331, 'ណ': 0.003926454487106883, '៧': 0.0004530524408200249, 'ក': 0.08789217351908483, '_': 3.775437006833541e-05, 'y': 7.550874013667082e-05, 'ទ': 0.04100124589421226, 'n': 0.00015101748027334164, '1': 0.00018877185034167705, 'd': 0.00022652622041001246, 'ឿ': 3.775437006833541e-05, 'l': 3.775437006833541e-05, 'k': 0.0003397893306150187, 'B': 0.0007928417714350436, 'ន': 0.07935968588364103, 'D': 0.0002642805904783479, 'ឥ': 0.0012081398421867331, 'M': 0.0009438592517083853, '!': 3.775437006833541e-05, 'ើ': 3.775437006833541e-05, 'ំ': 3.775437006833541e-05, 'អ': 0.04379506927926907, 'ឋ': 0.0002642805904783479, 'U': 0.0003020349605466833, 'Q': 3.775437006833541e-05, 'x': 3.775437006833541e-05, ':': 3.775437006833541e-05, 'h': 0.0003397893306150187, 'ូ': 3.775437006833541e-05, '០': 0.001283648582323404, 'c': 0.0003397893306150187, 'ៈ': 3.775437006833541e-05, 'j': 7.550874013667082e-05, 'é': 3.775437006833541e-05, '៨': 0.0003775437006833541, '។': 3.775437006833541e-05, '{': 3.775437006833541e-05, 'm': 3.775437006833541e-05, 'យ': 0.012194661532072337, 'q': 3.775437006833541e-05, 'ឬ': 3.775437006833541e-05, 'ែ': 3.775437006833541e-05, '៦': 0.0006795786612300374, 'ះ': 3.775437006833541e-05, 'p': 0.0003020349605466833, '៕': 3.775437006833541e-05, 'i': 0.000641824291161702, 'ៀ': 3.775437006833541e-05, 'K': 0.0002642805904783479, 'Y': 0.0003397893306150187, '”': 0.0002642805904783479, \"'\": 3.775437006833541e-05, '😭': 3.775437006833541e-05, 'ប': 0.09665118737493865, 'F': 0.000641824291161702, 'ជ': 0.037943141918677085, 's': 7.550874013667082e-05, '(': 3.775437006833541e-05}, 'E': {'៏': 0.0073243477932570696, 'v': 0.0002642805904783479, 'ិ': 0.008494733265375467, 'ឍ': 0.0005285611809566958, '៊': 3.775437006833541e-05, '៩': 0.0031713670857401745, '3': 3.775437006833541e-05, '២': 0.0008683505115717144, 'គ': 0.0029825952353984973, 'ង': 0.10371125457771738, '9': 0.00041529807075168953, 'ឈ': 3.775437006833541e-05, '៖': 7.550874013667082e-05, '$': 3.775437006833541e-05, 'g': 0.0007928417714350436, '+': 3.775437006833541e-05, 'ត': 0.037037037037037035, 'ល': 0.03866047494997546, 'ឃ': 0.0003397893306150187, 'Z': 7.550874013667082e-05, 'ព': 0.010306943028655566, 'ឆ': 3.775437006833541e-05, '6': 3.775437006833541e-05, 'ម': 0.029335145543096613, 'f': 3.775437006833541e-05, 'ស': 0.01925472873485106, 'H': 3.775437006833541e-05, 'ឲ': 3.775437006833541e-05, '=': 3.775437006833541e-05, 'W': 3.775437006833541e-05, '–': 3.775437006833541e-05, 'E': 0.00015101748027334164, 'T': 3.775437006833541e-05, 'ៗ': 3.775437006833541e-05, 'វ': 0.01249669649261902, '8': 0.00011326311020500623, '.': 0.0004530524408200249, '»': 3.775437006833541e-05, '់': 0.08823196284969985, '័': 7.550874013667082e-05, 'ឺ': 0.00641824291161702, 'ឦ': 3.775437006833541e-05, '៌': 0.0024540340544418017, '“': 3.775437006833541e-05, 'L': 0.00011326311020500623, 'V': 0.00011326311020500623, 'I': 0.00011326311020500623, 'ធ': 0.004530524408200249, '÷': 3.775437006833541e-05, 'O': 0.0003775437006833541, 'ុ': 0.004303998187790236, 'u': 0.00022652622041001246, '5': 0.00018877185034167705, 'ឩ': 3.775437006833541e-05, 'ឫ': 3.775437006833541e-05, 'É': 3.775437006833541e-05, 'ភ': 0.0009816136217767207, 'ដ': 0.0009438592517083853, 'ៅ': 0.022275078340317892, '៤': 0.0011703854721183977, '-': 3.775437006833541e-05, 'r': 0.000641824291161702, 't': 0.0009061048816400498, 'N': 0.00018877185034167705, '៉': 0.00011326311020500623, '%': 3.775437006833541e-05, 'ឱ': 3.775437006833541e-05, 'រ': 0.04198285951598898, 'J': 3.775437006833541e-05, 'ថ': 0.000641824291161702, 'e': 0.002151999093895118, 'ឯ': 0.0003775437006833541, ',': 0.0003397893306150187, '©': 3.775437006833541e-05, '…': 3.775437006833541e-05, 'ញ': 0.01555480046815419, 'A': 0.0003020349605466833, 'ួ': 0.00022652622041001246, 'ច': 0.013780345074942425, 'X': 0.00011326311020500623, '្': 3.775437006833541e-05, 'R': 7.550874013667082e-05, 'ហ': 0.0003775437006833541, 'ឡ': 7.550874013667082e-05, 'ផ': 3.775437006833541e-05, '4': 0.00011326311020500623, 'b': 7.550874013667082e-05, 'ោ': 0.0009061048816400498, 'ឧ': 3.775437006833541e-05, 'ា': 0.060067202778721636, 'C': 0.00015101748027334164, 'ឹ': 3.775437006833541e-05, '/': 3.775437006833541e-05, 'P': 0.00022652622041001246, '៍': 0.009136557556537169, 'ឌ': 0.0004908068108883604, '0': 0.0003397893306150187, 'z': 3.775437006833541e-05, '៣': 0.0009816136217767207, 'o': 0.0008683505115717144, 'ឭ': 3.775437006833541e-05, '«': 3.775437006833541e-05, '7': 7.550874013667082e-05, '’': 3.775437006833541e-05, '2': 0.00015101748027334164, 'ី': 0.0593876241174916, 'G': 0.00015101748027334164, '\"': 3.775437006833541e-05, '?': 3.775437006833541e-05, ')': 0.00015101748027334164, 'េ': 0.010306943028655566, 'ឪ': 3.775437006833541e-05, 'ៃ': 0.011326311020500622, 'w': 0.00015101748027334164, '១': 0.0020764903537584477, '៥': 0.0009061048816400498, 'a': 0.0007173330312983728, 'S': 0.00011326311020500623, 'ខ': 0.005965190470796995, 'ណ': 0.001925472873485106, '៧': 0.002491788424510137, 'ក': 0.05327141616642126, '_': 3.775437006833541e-05, 'y': 0.0006040699210933666, 'ទ': 0.004417261297995243, 'n': 0.0008683505115717144, '1': 0.00018877185034167705, 'd': 0.0005285611809566958, 'ឿ': 0.0003397893306150187, 'l': 0.0007550874013667082, 'k': 0.0007550874013667082, 'B': 0.0003397893306150187, 'ន': 0.10295616717635066, 'D': 0.00018877185034167705, 'ឥ': 3.775437006833541e-05, 'M': 3.775437006833541e-05, '!': 3.775437006833541e-05, 'ើ': 0.01340280137425907, 'ំ': 0.024464831804281346, 'អ': 0.0016234379129384227, 'ឋ': 0.0026805602748518143, 'U': 7.550874013667082e-05, 'Q': 3.775437006833541e-05, 'x': 3.775437006833541e-05, ':': 0.00015101748027334164, 'h': 0.0007550874013667082, 'ូ': 0.0025295427945784725, '០': 0.003775437006833541, 'c': 0.00022652622041001246, 'ៈ': 0.003435647676218522, 'j': 3.775437006833541e-05, 'é': 3.775437006833541e-05, '៨': 0.0006040699210933666, '។': 0.00018877185034167705, '{': 3.775437006833541e-05, 'm': 0.0006795786612300374, 'យ': 0.05791520368482652, 'q': 3.775437006833541e-05, 'ឬ': 3.775437006833541e-05, 'ែ': 0.01200588968173066, '៦': 0.0010193679918450561, 'ះ': 0.03971759731188885, 'p': 0.0003775437006833541, '៕': 0.0003397893306150187, 'i': 0.0005663155510250311, 'ៀ': 3.775437006833541e-05, 'K': 0.00015101748027334164, 'Y': 3.775437006833541e-05, '”': 0.00011326311020500623, \"'\": 3.775437006833541e-05, '😭': 3.775437006833541e-05, 'ប': 0.014459923736172463, 'F': 7.550874013667082e-05, 'ជ': 0.0007173330312983728, 's': 0.0007928417714350436, '(': 3.775437006833541e-05}, 'M': {'៏': 0.0001326207180085673, 'v': 0.00010609657440685384, 'ិ': 0.03160351710144159, 'ឍ': 0.0005570070156359826, '៊': 0.004389745766083577, '៩': 0.0001326207180085673, '3': 3.978621540257019e-05, '២': 0.00011935864620771057, 'គ': 0.007148256700661777, 'ង': 0.019773749055077384, '9': 2.652414360171346e-05, 'ឈ': 0.0005702690874368394, '៖': 1.326207180085673e-05, '$': 1.326207180085673e-05, 'g': 0.00023871729241542114, '+': 1.326207180085673e-05, 'ត': 0.032372717265891277, 'ល': 0.02546317785764492, 'ឃ': 0.0008620346670556874, 'Z': 1.326207180085673e-05, 'ព': 0.014203678898717558, 'ឆ': 0.0004376483694282721, '6': 3.978621540257019e-05, 'ម': 0.028009495643409415, 'f': 9.28345026059971e-05, 'ស': 0.02883174409506253, 'H': 2.652414360171346e-05, 'ឲ': 0.0010609657440685384, '=': 1.326207180085673e-05, 'W': 2.652414360171346e-05, '–': 1.326207180085673e-05, 'E': 7.957243080514038e-05, 'T': 0.00021219314881370767, 'ៗ': 1.326207180085673e-05, 'វ': 0.01172367147195735, '8': 3.978621540257019e-05, '.': 0.0001326207180085673, '»': 1.326207180085673e-05, '់': 0.007519594711085766, '័': 0.004774345848308423, 'ឺ': 0.0010874898876702518, 'ឦ': 1.326207180085673e-05, '៌': 0.0006631035900428365, '“': 1.326207180085673e-05, 'L': 7.957243080514038e-05, 'V': 0.00014588278980942404, 'I': 0.00021219314881370767, 'ធ': 0.00636579446441123, '÷': 1.326207180085673e-05, 'O': 0.00010609657440685384, 'ុ': 0.031404586024428735, 'u': 0.0005304828720342692, '5': 7.957243080514038e-05, 'ឩ': 1.326207180085673e-05, 'ឫ': 1.326207180085673e-05, 'É': 1.326207180085673e-05, 'ភ': 0.00636579446441123, 'ដ': 0.010052650425049402, 'ៅ': 0.002652414360171346, '៤': 0.00010609657440685384, '-': 1.326207180085673e-05, 'r': 0.0008620346670556874, 't': 0.0006631035900428365, 'N': 0.00014588278980942404, '៉': 0.005981194382186385, '%': 1.326207180085673e-05, 'ឱ': 0.0004509104412291288, 'រ': 0.06502393803960055, 'J': 1.326207180085673e-05, 'ថ': 0.0036735938888373143, 'e': 0.0016047106879036642, 'ឯ': 0.0004774345848308423, ',': 0.0003713380104239884, '©': 1.326207180085673e-05, '…': 1.326207180085673e-05, 'ញ': 0.006883015264644643, 'A': 0.0001856690052119942, 'ួ': 0.01602058273543493, 'ច': 0.011975650836173628, 'X': 1.326207180085673e-05, '្': 0.14666525204567457, 'R': 0.0001326207180085673, 'ហ': 0.006153601315597523, 'ឡ': 0.0024269591395567817, 'ផ': 0.0023208625651499276, '4': 1.326207180085673e-05, 'b': 0.0002652414360171346, 'ោ': 0.01905759717783112, 'ឧ': 0.0002254552206145644, 'ា': 0.1199289152951474, 'C': 9.28345026059971e-05, 'ឹ': 0.007890932721509755, '/': 1.326207180085673e-05, 'P': 7.957243080514038e-05, '៍': 0.00019893107701285094, 'ឌ': 0.0014190416826916701, '0': 0.00017240693341113747, 'z': 0.00015914486161028077, '៣': 0.00014588278980942404, 'o': 0.001869952123920799, 'ឭ': 1.326207180085673e-05, '«': 1.326207180085673e-05, '7': 0.0001326207180085673, '’': 1.326207180085673e-05, '2': 0.00010609657440685384, 'ី': 0.00758590507009005, 'G': 3.978621540257019e-05, '\"': 1.326207180085673e-05, '?': 1.326207180085673e-05, ')': 1.326207180085673e-05, 'េ': 0.026364998740103177, 'ឪ': 1.326207180085673e-05, 'ៃ': 0.0021882418471413606, 'w': 0.0001856690052119942, '១': 0.0012201106056788191, '៥': 0.0001856690052119942, 'a': 0.0019495245547259392, 'S': 6.631035900428365e-05, 'ខ': 0.0040316698274604455, 'ណ': 0.011365595533334218, '៧': 7.957243080514038e-05, 'ក': 0.03442833839502407, '_': 1.326207180085673e-05, 'y': 0.00017240693341113747, 'ទ': 0.0146811134835484, 'n': 0.0012333726774796758, '1': 0.0001326207180085673, 'd': 0.0005304828720342692, 'ឿ': 0.0016975451905096614, 'l': 0.0007957243080514038, 'k': 0.00014588278980942404, 'B': 6.631035900428365e-05, 'ន': 0.04185509860350384, 'D': 9.28345026059971e-05, 'ឥ': 1.326207180085673e-05, 'M': 3.978621540257019e-05, '!': 1.326207180085673e-05, 'ើ': 0.01667042425367691, 'ំ': 0.02051642507592536, 'អ': 0.004482580268689575, 'ឋ': 0.0020290969855310797, 'U': 3.978621540257019e-05, 'Q': 1.326207180085673e-05, 'x': 0.00010609657440685384, ':': 0.00011935864620771057, 'h': 0.0006365794464411231, 'ូ': 0.018195562510775434, '០': 0.001976048698327653, 'c': 0.0005172208002334125, 'ៈ': 0.00042438629762741535, 'j': 3.978621540257019e-05, 'é': 1.326207180085673e-05, '៨': 0.00010609657440685384, '។': 1.326207180085673e-05, '{': 1.326207180085673e-05, 'm': 0.0003580759386231317, 'យ': 0.00977414691723141, 'q': 1.326207180085673e-05, 'ឬ': 1.326207180085673e-05, 'ែ': 0.01698871397689747, '៦': 0.00017240693341113747, 'ះ': 0.002745248862777343, 'p': 0.0003978621540257019, '៕': 1.326207180085673e-05, 'i': 0.0012864209646831029, 'ៀ': 0.004973276925321274, 'K': 6.631035900428365e-05, 'Y': 5.304828720342692e-05, '”': 1.326207180085673e-05, \"'\": 1.326207180085673e-05, '😭': 1.326207180085673e-05, 'ប': 0.03128522737822102, 'F': 0.0001856690052119942, 'ជ': 0.011405381748736787, 's': 0.0007161518772462634, '(': 1.326207180085673e-05}, 'S': {'៏': 0.0005555555555555556, 'v': 0.0011111111111111111, 'ិ': 0.0005555555555555556, 'ឍ': 0.0005555555555555556, '៊': 0.0005555555555555556, '៩': 0.0038888888888888888, '3': 0.0016666666666666668, '២': 0.029444444444444443, 'គ': 0.0011111111111111111, 'ង': 0.0022222222222222222, '9': 0.0011111111111111111, 'ឈ': 0.0005555555555555556, '៖': 0.065, '$': 0.0011111111111111111, 'g': 0.0005555555555555556, '+': 0.0011111111111111111, 'ត': 0.0033333333333333335, 'ល': 0.0022222222222222222, 'ឃ': 0.0005555555555555556, 'Z': 0.0011111111111111111, 'ព': 0.006666666666666667, 'ឆ': 0.0005555555555555556, '6': 0.0011111111111111111, 'ម': 0.005, 'f': 0.0005555555555555556, 'ស': 0.015, 'H': 0.0005555555555555556, 'ឲ': 0.0005555555555555556, '=': 0.0011111111111111111, 'W': 0.0005555555555555556, '–': 0.0011111111111111111, 'E': 0.0011111111111111111, 'T': 0.0005555555555555556, 'ៗ': 0.055, 'វ': 0.0005555555555555556, '8': 0.0011111111111111111, '.': 0.012222222222222223, '»': 0.01888888888888889, '់': 0.0005555555555555556, '័': 0.0005555555555555556, 'ឺ': 0.0005555555555555556, 'ឦ': 0.0005555555555555556, '៌': 0.0005555555555555556, '“': 0.01, 'L': 0.0005555555555555556, 'V': 0.0022222222222222222, 'I': 0.0005555555555555556, 'ធ': 0.0005555555555555556, '÷': 0.0011111111111111111, 'O': 0.0005555555555555556, 'ុ': 0.0005555555555555556, 'u': 0.0005555555555555556, '5': 0.005555555555555556, 'ឩ': 0.0005555555555555556, 'ឫ': 0.0005555555555555556, 'É': 0.0005555555555555556, 'ភ': 0.0005555555555555556, 'ដ': 0.0005555555555555556, 'ៅ': 0.0005555555555555556, '៤': 0.01, '-': 0.02277777777777778, 'r': 0.0005555555555555556, 't': 0.0005555555555555556, 'N': 0.0005555555555555556, '៉': 0.0005555555555555556, '%': 0.005, 'ឱ': 0.0005555555555555556, 'រ': 0.0022222222222222222, 'J': 0.0005555555555555556, 'ថ': 0.0005555555555555556, 'e': 0.0005555555555555556, 'ឯ': 0.0022222222222222222, ',': 0.02666666666666667, '©': 0.0016666666666666668, '…': 0.021666666666666667, 'ញ': 0.0005555555555555556, 'A': 0.0011111111111111111, 'ួ': 0.0005555555555555556, 'ច': 0.006111111111111111, 'X': 0.0005555555555555556, '្': 0.0005555555555555556, 'R': 0.0005555555555555556, 'ហ': 0.0005555555555555556, 'ឡ': 0.0005555555555555556, 'ផ': 0.0005555555555555556, '4': 0.0011111111111111111, 'b': 0.0005555555555555556, 'ោ': 0.0005555555555555556, 'ឧ': 0.0005555555555555556, 'ា': 0.0005555555555555556, 'C': 0.0005555555555555556, 'ឹ': 0.0005555555555555556, '/': 0.002777777777777778, 'P': 0.0005555555555555556, '៍': 0.0005555555555555556, 'ឌ': 0.0005555555555555556, '0': 0.0005555555555555556, 'z': 0.0005555555555555556, '៣': 0.012222222222222223, 'o': 0.0005555555555555556, 'ឭ': 0.0005555555555555556, '«': 0.017222222222222222, '7': 0.0011111111111111111, '’': 0.0011111111111111111, '2': 0.0005555555555555556, 'ី': 0.0005555555555555556, 'G': 0.006111111111111111, '\"': 0.0011111111111111111, '?': 0.01611111111111111, ')': 0.043333333333333335, 'េ': 0.0005555555555555556, 'ឪ': 0.0005555555555555556, 'ៃ': 0.0005555555555555556, 'w': 0.0005555555555555556, '១': 0.028333333333333332, '៥': 0.012777777777777779, 'a': 0.0005555555555555556, 'S': 0.0005555555555555556, 'ខ': 0.0016666666666666668, 'ណ': 0.0005555555555555556, '៧': 0.0077777777777777776, 'ក': 0.005, '_': 0.0016666666666666668, 'y': 0.0005555555555555556, 'ទ': 0.0005555555555555556, 'n': 0.0005555555555555556, '1': 0.0044444444444444444, 'd': 0.0005555555555555556, 'ឿ': 0.0005555555555555556, 'l': 0.0005555555555555556, 'k': 0.0005555555555555556, 'B': 0.0005555555555555556, 'ន': 0.002777777777777778, 'D': 0.0005555555555555556, 'ឥ': 0.0005555555555555556, 'M': 0.0005555555555555556, '!': 0.011111111111111112, 'ើ': 0.0005555555555555556, 'ំ': 0.0005555555555555556, 'អ': 0.0011111111111111111, 'ឋ': 0.0005555555555555556, 'U': 0.0005555555555555556, 'Q': 0.0005555555555555556, 'x': 0.0005555555555555556, ':': 0.01, 'h': 0.0005555555555555556, 'ូ': 0.0005555555555555556, '០': 0.0011111111111111111, 'c': 0.0005555555555555556, 'ៈ': 0.0005555555555555556, 'j': 0.0005555555555555556, 'é': 0.0005555555555555556, '៨': 0.008333333333333333, '។': 0.35777777777777775, '{': 0.0005555555555555556, 'm': 0.0005555555555555556, 'យ': 0.0022222222222222222, 'q': 0.0005555555555555556, 'ឬ': 0.02666666666666667, 'ែ': 0.0005555555555555556, '៦': 0.007222222222222222, 'ះ': 0.0005555555555555556, 'p': 0.0005555555555555556, '៕': 0.034444444444444444, 'i': 0.0011111111111111111, 'ៀ': 0.0005555555555555556, 'K': 0.0005555555555555556, 'Y': 0.0005555555555555556, '”': 0.01, \"'\": 0.0005555555555555556, '😭': 0.0005555555555555556, 'ប': 0.0011111111111111111, 'F': 0.0005555555555555556, 'ជ': 0.0005555555555555556, 's': 0.0011111111111111111, '(': 0.04833333333333333}}\n",
            "trans_prob {'B': {'B': 0.0, 'E': 0.11998338807716993, 'M': 0.8800166119228301, 'S': 0.0}, 'E': {'B': 0.9377430437573149, 'E': 0.0, 'M': 0.0, 'S': 0.05968965907803828}, 'M': {'B': 0.0, 'E': 0.3091256316061695, 'M': 0.6908743683938304, 'S': 0.0}, 'S': {'B': 0.5, 'E': 0.0, 'M': 0.0, 'S': 0.09388888888888888}}\n",
            "- Preprocess: len of observation: 183 ['អ', 'ា', 'ម', 'េ', 'រ', 'ិ', 'ក', 'ព', '្', 'រ', 'ម', 'ផ', '្', 'ត', 'ល', '់', 'ស', 'េ', 'វ', 'ា', 'គ', 'ា', 'ំ', 'ទ', '្', 'រ', 'ប', 'ច', '្', 'ច', 'េ', 'ក', 'ទ', 'េ', 'ស', 'ដ', 'ល', '់', 'យ', 'ន', '្', 'ត', 'ហ', 'ោ', 'ះ', 'ច', 'ម', '្', 'ប', 'ា', 'ំ', 'ង', 'F', '-', '1', '6', 'ប', '៉', 'ា', 'គ', 'ី', 'ស', '្', 'ថ', 'ា', 'ន', 'ន', 'ិ', 'ង', 'ល', 'ក', '់', 'គ', '្', 'រ', 'ឿ', 'ង', 'ប', 'ន', '្', 'ល', 'ា', 'ស', '់', 'យ', 'ន', '្', 'ត', 'ហ', 'ោ', 'ះ', 'យ', 'ោ', 'ធ', 'ា', 'ឲ', '្', 'យ', 'ឥ', 'ណ', '្', 'ឌ', 'ា', '2', '7', ',', 'J', 'u', 'l', '2', '0', '1', '9', ',', '1', '0', ':', '3', '0', 'p', 'm']\n",
            "word_seq len test: 183\n",
            "-word_sequence[0] អាមេរិក * ព្រម * ផ្តល់ * សេវា * គាំ * ទ្រ * បច្ចេក * ទេស * ដល់ * យន្ត * ហោះ * ចម្បាំង * F * - * 1 * 6 * ប៉ាគី * ស្ថាន * និង * លក់ * គ្រឿង * បន្លាស់ * យន្ត * ហោះ * យោធាឲ្យ * ឥណ្ឌា2 * 7 * , * Jul * 2019 * , * 1 * 0 * : * 3 * 0pm\n",
            "0-hstates0 ['B', 'M', 'M', 'M', 'M', 'M', 'E', 'B', 'M', 'M', 'E', 'B', 'M', 'M', 'M', 'E', 'B', 'M', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'M', 'M', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'M', 'M', 'M', 'M', 'E', 'S', 'S', 'S', 'S', 'B', 'M', 'M', 'M', 'E', 'B', 'M', 'M', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'M', 'M', 'E', 'B', 'M', 'M', 'M', 'M', 'M', 'E', 'B', 'M', 'M', 'E', 'B', 'M', 'E', 'B', 'M', 'M', 'M', 'M', 'M', 'E', 'B', 'M', 'M', 'M', 'M', 'E', 'S', 'S', 'B', 'M', 'E', 'B', 'M', 'M', 'E', 'S', 'S', 'S', 'S', 'S', 'B', 'M', 'E']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEdxP6Pt8yiC",
        "colab_type": "code",
        "outputId": "0eec9ad6-0e2b-45e0-ea68-8510756ef75c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# compare to segmentation text\n",
        "!grep \"អាមេរិក ព្រម ផ្តល់\" kh_data_100/3135*_seg.txt\n",
        "#!head kh_data_100/313502_orig.txt\n",
        "!head kh_data_100/313502_seg.txt\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kh_data_100/313502_seg.txt: អាមេរិក ព្រម ផ្តល់ សេវា គាំទ្រ បច្ចេកទេស ដល់ យន្តហោះ ចម្បាំង F - 16 ប៉ាគីស្ថាន និង លក់ គ្រឿង បន្លាស់ យន្តហោះ យោធា ឲ្យ ឥណ្ឌា 27, Jul 2019 , 10:30 pm \n",
            " អាមេរិក ព្រម ផ្តល់ សេវា គាំទ្រ បច្ចេកទេស ដល់ យន្តហោះ ចម្បាំង F - 16 ប៉ាគីស្ថាន និង លក់ គ្រឿង បន្លាស់ យន្តហោះ យោធា ឲ្យ ឥណ្ឌា 27, Jul 2019 , 10:30 pm \n",
            "រដ្ឋាភិបាល ក្រុង វ៉ាស៊ីនតោន បាន សម្រេច អនុម័ត លក់ គ្រឿង បន្លាស់ និង ផ្តល់ ការគាំទ្រ ផ្នែក បច្ចេកទេស សម្រាប់ យន្តហោះ យោធា F - 16 របស់ ប៉ាគីស្ថាន និង យន្តហោះ ដឹកជញ្ជូន C - 17 របស់ ឥណ្ឌា ។\n",
            " យន្តហោះ ចម្បាំង F - 16 របស់ ប៉ាគីស្ថាន និង យន្តហោះ ដឹកជញ្ជូន យោធា C - 17 របស់ ឥណ្ឌា នឹង ទទួលបាន ការ សេវា គាំទ្រ បច្ចេកទេស និង ភស្តុភារ ពីសំណាក់ រដ្ឋបាល អាមេរិក បន្ទាប់ពី ក្រសួង ការបរទេស អាមេរិក សម្រេច អនុម័ត គម្រោង មាន តម្លៃ ៨០០ លាន ដុល្លារ អាមេរិក នៅ ថ្ងៃសុក្រ ទី ២៦ ខែកក្កដា ។\n",
            " បើ តាម ទីភ្នាក់ងារ ព័ត៌មាន AFP អាមេរិក សម្រេច យល់ព្រម ផ្តល់ សេវា គាំទ្រ បច្ចេកទេស និង ភស្តុភារ ដល់ យន្តហោះ F - 16 ដែល ប៉ាគីស្ថាន បាន ទិញ ពី អាមេរិក ។\n",
            " ការសម្រេច នេះ  ធ្វើឡើង ត្រឹម ប៉ុន្មាន ថ្ងៃ ប៉ុណ្ណោះ ក្រោយ ទស្សនកិច្ច របស់ នាយ រដ្ឋមន្ត្រី ប៉ាគីស្ថាន លោក Imran Khan នៅ អាមេរិក និង ជួប ជាមួយ លោក  ដូណាល់ ត្រាំ ដែរ ។\n",
            " « ការអនុម័ត ផ្តល់ សេវា បច្ចេកទេស និង ភស្តុភារ ទាំង នេះ នឹង គាំទ្រ គោលនយោបាយ ការបរទេស អាមេរិក  និង ធានា សន្តិសុខ ជាតិ  ដោយ ការពារ បច្ចេកវិទ្យា អាមេរិក តាមរយៈ ការបន្ត រក្សា វត្តមាន របស់ អ្នកបច្ចេកទេស អាមេរិក នៅ គាំទ្រ ច្ចេកទេស ក្នុង ការប្រើប្រាស់ យន្តហោះ ចម្បាំង F - 16 របស់ ប៉ាគីស្ថាន » ។\n",
            " ក្រសួង ការបរទេស អាមេរិក បញ្ជាក់ ដូច្នេះ ។\n",
            " ចំណែក ក្រសួង ការបរទេស អាមេរិក ក៏ សម្រេច អនុម័ត លក់ គ្រឿង បន្លាស់ និង ផ្តល់ សេវា ហ្វឹកហ្វឺន ផ្សេង ទៀត ដើម្បី គាំទ្រ កងទ័ព ឥណ្ឌា ក្នុង ការប្រើប្រាស់ យន្តហោះ ដឹកជញ្ជូន យោធា C - 17 ។\n",
            " គម្រោង នេះ មាន តម្លៃ សរុប ប្រមាណ ៦៧០ លាន ដុល្លារ ឯណោះ ។\n",
            " « ឥណ្ឌា ត្រូវការ ការគាំទ្រ បច្ចេកទេស បន្ត ទៀត ដើម្បី ធានា ការត្រៀមខ្លួន ប្រតិបត្តិការ  និង សមត្ថភាព នានា ក្នុង ការផ្តល់ ការគាំទ្រ ប្រតិបត្តិការ ផ្តល់ ជំនួយ មនុស្សធម៌  និង សម្រួល ដល់ ការដោះស្រាយ បញ្ហា គ្រោះមហន្តរាយ នៅក្នុង តំបន់ » ។\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isTnzrC-_-eo",
        "colab_type": "text"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0sDq1FdAjpc",
        "colab_type": "text"
      },
      "source": [
        "### Custom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81hZo3G95_bQ",
        "colab_type": "code",
        "outputId": "a579bfec-6514-4c7d-d439-1ac909b2ee19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "# Custom validation\n",
        "#prediction = [1,0,0,1,1,1,1,0,1,0]\n",
        "#correct    = [1,0,0,1,1,1,0,0,1,0]\n",
        "prediction = [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0]\n",
        "correct =    [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0]\n",
        "\n",
        "pstr = \"\".join(str(i) for i in prediction)\n",
        "cstr = \"\".join(str(i) for i in correct)\n",
        "print(\"P\",pstr)\n",
        "print(\"C\",cstr)\n",
        "#pl = pstr.split('1')\n",
        "\n",
        "\n",
        "def calc_perf(corrects, predictions): # list of 0/1\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "  n_correct = 0\n",
        "  n_incorrect = 0\n",
        "  total_char = 0\n",
        "  total_word = 0\n",
        "  n_correct_word = 0\n",
        "  \n",
        "  print(\"size of input:\", len(predictions), \"ground truth:\", len(corrects))\n",
        "  if len(predictions) != len(corrects): return 0\n",
        "  \n",
        "  for i, prediction in enumerate(predictions):\n",
        "    correct = corrects[i]\n",
        "    zipped = list(zip(prediction, correct))    \n",
        "    tp +=        len([1 for l, c in zipped if l == c and l == 1])\n",
        "    fp +=        len([1 for l, c in zipped if l == 1 and c == 0])\n",
        "    fn +=        len([1 for l, c in zipped if l == 0 and c == 1])\n",
        "    n_incorrect += len([1 for l, c in zipped if l != c])\n",
        "    n_correct   += len([1 for l, c in zipped if l == c])\n",
        "    #n_correct_word += len([1 for l,c in zipped if l==1 and c==1]) # not account for prediction=1 and correct=0\n",
        "    #n_incorrect_word += len([1 for l,c in zipped if l==0 and c==1]) # missing other way around\n",
        "    total_word += len([_ for l in correct if l==1])\n",
        "    total_char += len(prediction)\n",
        "    #print(\"len correct\", len(correct), \" incorrect count:\", n_incorrect)\n",
        "    # count good word\n",
        "    n_correct_word += count_correct_word(correct, prediction)\n",
        "  \n",
        "  print(\"Total char:\", str(total_char), \" total word:\", str(total_word), \"avg char/word:\", str(total_char/total_word))\n",
        "  print(\"Correct word:\" + str(n_correct_word), \" incorrect word:\", str(total_word - n_correct_word), \"word accuracy:\", n_correct_word/total_word) \n",
        "  \n",
        "  precision = tp/(tp+fp)\n",
        "  recall = tp/(tp+fn)\n",
        "  F1 = 2 * (precision * recall) / (precision + recall)\n",
        "  print(\"Precision:\\t\" + str(precision), \"tp:\", tp, \"fp:\", fp)\n",
        "  print(\"Recall:\\t\\t\" + str(recall), \"fn:\",fn)\n",
        "  print(\"F1-score:\\t\" + str(F1))\n",
        "  print(\"Accuracy:\\t\" + str(n_correct/(n_correct+n_incorrect))) \n",
        "  \n",
        "def count_correct_word(correct, prediction):\n",
        "  s = \"\"\n",
        "  for i in range(len(correct)):\n",
        "    s += \"%3s\" %str(i)\n",
        "  #print(\"prediction:\", prediction)\n",
        "  #print(\"   correct:\", correct)\n",
        "  #print(\"       str:\", s)\n",
        "  B=False\n",
        "  correct_count = 0\n",
        "  for i,c in enumerate(correct):\n",
        "    p = prediction[i]\n",
        "    nextc = -1\n",
        "    if i < len(correct)-1: \n",
        "      nextc = correct[i+1]\n",
        "    if c==1 and p==1:\n",
        "      B = True\n",
        "      correct_count += 1\n",
        "      #print(i,\"Begin word corect\", correct_count)\n",
        "    if p==0 and c==1 and B:\n",
        "      B = False\n",
        "      correct_count -= 1\n",
        "      #print(i, \"too long\")\n",
        "    if c==0 and p==1: #incorrect\n",
        "      if B: \n",
        "        correct_count -= 1\n",
        "        #print(i,\"bad word\", correct_count)\n",
        "        B = False\n",
        "  return correct_count\n",
        "\n",
        "correct_count = count_correct_word(prediction, correct)\n",
        "print(\"correct count\", correct_count)  \n",
        "calc_perf([correct], [prediction])\n",
        "\n",
        "from sklearn.metrics import classification_report \n",
        "print(classification_report(correct, prediction, target_names=[\"0\",\"1\"]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P 1000000100010000100010010010000010010010001001000000111110000100001001001000010000001000100100000010000011100100011111100\n",
            "C 1000000100010000100010000010000000010010000001000000111010000000001001001000010000001000000100010010000100100100011000010\n",
            "correct count 15\n",
            "size of input: 1 ground truth: 1\n",
            "Total char: 121  total word: 27 avg char/word: 4.481481481481482\n",
            "Correct word:15  incorrect word: 12 word accuracy: 0.5555555555555556\n",
            "Precision:\t0.6666666666666666 tp: 24 fp: 12\n",
            "Recall:\t\t0.8888888888888888 fn: 3\n",
            "F1-score:\t0.761904761904762\n",
            "Accuracy:\t0.8760330578512396\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.87      0.92        94\n",
            "           1       0.67      0.89      0.76        27\n",
            "\n",
            "    accuracy                           0.88       121\n",
            "   macro avg       0.82      0.88      0.84       121\n",
            "weighted avg       0.90      0.88      0.88       121\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0DmJUxmAnGN",
        "colab_type": "text"
      },
      "source": [
        "### Check Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBfTYO2MLnxY",
        "colab_type": "code",
        "outputId": "e2e79db7-0fae-46b6-ae3d-ac784a74b6dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "# Check accuracy\n",
        "test_labels = [] # predicted result\n",
        "g_labels = []    # ground thruth\n",
        "for i, el in enumerate(test_states):\n",
        "  test_label = []\n",
        "  for c in el: #string\n",
        "    v = 1 if c in \"BS\" else 0\n",
        "    test_label.append(v)\n",
        "  g_label = []\n",
        "  for c in o_hstate[i]:\n",
        "    v = 1 if c in \"BS\" else 0\n",
        "    g_label.append(v)\n",
        "  if len(test_label) != len(g_label):\n",
        "    print(i,\"test_label len:\", len(test_label), test_label)\n",
        "    print(i,\"   g_label len:\", len(g_label), g_label)\n",
        "    print(\"--- Not matching length ---- observation:\", observations[i])\n",
        "  test_labels.append(test_label)\n",
        "  g_labels.append(g_label)\n",
        "  \n",
        "# check custom metric\n",
        "#calc_perf(test_labels[0:1], g_labels[0:1])\n",
        "calc_perf(test_labels, g_labels)\n",
        "\n",
        "flat_predicts = [item for t in test_labels for item in t]\n",
        "flat_true = [item for t in g_labels for item in t]\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report \n",
        "print(classification_report(flat_predicts, flat_true, \n",
        "      target_names=[\"0\",\"1\"]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of input: 183 ground truth: 183\n",
            "Total char: 32179  total word: 7072 avg char/word: 4.550197963800905\n",
            "Correct word:3099  incorrect word: 3973 word accuracy: 0.43820701357466063\n",
            "Precision:\t0.49704092689839124 tp: 5963 fp: 6034\n",
            "Recall:\t\t0.8431843891402715 fn: 1109\n",
            "F1-score:\t0.625412973936756\n",
            "Accuracy:\t0.7780229342117531\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.76      0.84     25107\n",
            "           1       0.50      0.84      0.63      7072\n",
            "\n",
            "    accuracy                           0.78     32179\n",
            "   macro avg       0.72      0.80      0.73     32179\n",
            "weighted avg       0.85      0.78      0.79     32179\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}